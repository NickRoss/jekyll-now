---
layout: projects 
title: GKE and Google
mathjax: true
---

Currently have k8s set up with istio, grafana, promethus and kiali. A few notes

1. To authenticate you need to go `glcoud init` (may not be neccessary), `gcloud container clusters list` to get the cluster name and then `gcloud container clusters get-credentials <cluster-name> --region <region>` in order to connect it to `kubectl`
1. To check the status of istio, use `istio proxy-status` and, assuming it goes through, the following should work: 
	a. `istioctl dashboard grafana`
	b. `istioctl dashboard kiali`


Things I want to see when evaluating a cluster. When using containers this information should be by all of pod/node/machine, which	ever is applicable.

1. Networking:
	a. Ingress
	b. Egress
1. CPU Usage 
1. Memory Usage 
1. Hard Drive space usage

Additionally if this is using some type of HTTP traffic, I want to see breakdown by return (200/404) by end-point. This should be broken down by pod/node/machine when applicable.

---
## Getting Locust running on an instance + some Results
---

Spun up a C5A.xlarge. Note that AWS "high" networking = 1 gigabit a second and that I made the box available everywhere with essentially zero inbound/outbound limits. Spun up as Ubuntu box and did the following on the box

```
sudo apt update
sudo apt install --yes python3 python3-pip
pip3 install locust requests
sudo ln -s /usr/bin/python3 /usr/bin/python
mkdir ~/put_json
```

I added the box to `~/.ssh/config` as `LoadTest` and then copied up the locust file:

`scp locustfile.py LoadTest:/home/ubuntu/.`
`scp put_json/*.json LoadTest:/home/ubuntu/put_json/.`

ssh'd back in : `locust --host https://game-themeta.com`

Note that I had to exit/come back in for machine to find the `locust` bin.

If you get an error about number of open files, then you need to run the following `ulimit -n 65535` which is _session_ based, so has to be changed each time.	


